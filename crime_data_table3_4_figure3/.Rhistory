df_freq$bayes.s.d. <- paste0(sprintf("%.4f",df_freq$bayes.s.d.),
"(",
sprintf("%.4f",df_freq$bayes.mc.se),
")")
df_freq <- df_freq %>%
select(-c("mc.se","bayes.mc.se"))
df_freq <- df_freq %>%
dplyr::select(-c("mc.se","bayes.mc.se"))
colnames(df_freq) <- c("sample size",
"parameter",
"point.est",
"$\\widehat{\\text{s.d.}}$",
"s.d.",
"point.est",
"$\\widehat{\\text{s.d.}}$",
"s.d.")
kable(df_freq,
booktab = TRUE,
escape = FALSE,
caption = "Frequentist and Bayesian inference for experiment (E1) across 1000 Monte Carlo replicates. point.est stands for mean of 1000 point estimations. $\\widehat{\\text{s.d.}}$ and s.d. stand for mean of the corresponding estimated standard deviations and empirical standard deviations respectively. Numbers in parentheses are 100 Ã— (Monte Carlo standard errors) associated with the averages.",
label = "E1tab") %>%
collapse_rows(1) %>%
row_spec(row = 0, align = "c") %>%
add_header_above(c(" ",
" ",
"frequentist" = 3,
"Bayesian" = 3))
tab <- kable(df_freq,
booktab = TRUE,
escape = FALSE,
caption = "Frequentist and Bayesian inference for experiment (E1) across 1000 Monte Carlo replicates. point.est stands for mean of 1000 point estimations. $\\widehat{\\text{s.d.}}$ and s.d. stand for mean of the corresponding estimated standard deviations and empirical standard deviations respectively. Numbers in parentheses are Monte Carlo standard errors associated with the averages.",
label = "E1tab",
format = 'latex') %>%
collapse_rows(1) %>%
row_spec(row = 0, align = "c") %>%
add_header_above(c(" ",
" ",
"frequentist" = 3,
"Bayesian" = 3))
tab
writeLines(tab, 'tab.tex')
setwd("~/OneDrive_UofSC/Research/FG_revision/flexible_Gumbel_codes/table2")
setwd("~/OneDrive_UofSC/Research/FG_revision/flexible_Gumbel_codes/table2")
rm(list=ls())
library(mixtools)
source("../density.R")
source("../EM_v1.0.R")
source("../Bayes_MH_v4.0.r")
df1 <- read.csv("data_toy.csv")
ECM_est <- em(df1$change,w1=0.5,loc=0,scale1=5,scale2=4)
ECM_SE <- sandwichvar_se(df1$change,
ECM_est$w1,
ECM_est$loc,
ECM_est$scale1,
ECM_est$scale2)
est_normal <- normalmixEM(df1$change, lambda = .5, mu = c(-0.1,0.1), sigma = c(2.9,3.1))
saveRDS(ECM_est,"ECM_est.rds")
saveRDS(ECM_SE,"ECM_SE.rds")
saveRDS(est_normal,"normal_mixture_EM.rds")
loglik_value <- sum(mixture_gumbel_pdf(x = df1$change,
w1 = ECM_est$w1,
loc = ECM_est$loc,
scale1 = ECM_est$scale1,
scale2 = ECM_est$scale2,
log=TRUE))
AIC_value <- -2*loglik_value + 2*(4)
AIC_value
4*log(length(df1$change))
BIC_value <- -2*loglik_value + 4*log(length(df1$change))
BIC_value
est_normal
AIC_value <- -2*est_normal$loglik + 2*(5)
AIC_value
BIC_value <- -2*est_normal$loglik + 5*log(length(df1$change))
BIC_value
setwd("~/OneDrive_UofSC/Research/FG_revision/flexible_Gumbel_codes/table2")
rm(list=ls())
library(rstan)
library(R2jags)
df1 <- read.csv("data_toy.csv")
y <- df1$change
N <- nrow(df1)
dat <- list(N = N,
y = y)
##rstan
fit <- stan(file = 'model_fitting.stan',
data = dat,
iter = 100000,
seed = 100,
chains = 15)
options(mc.cores = parallel::detectCores())
##rstan
fit <- stan(file = 'model_fitting.stan',
data = dat,
iter = 100000,
seed = 100,
chains = 15)
summary(fit)
sum_fit <- summary(fit)
sum_fit$summary
sum_fit$summary$mean
sum_fit$summary$mean
sum_fit$summary[,1]
sum_fit$summary[,1][1:4]
sum_fit$summary[,6][1:4]
#rstan::traceplot(fit)
saveRDS(fit,"stanoutput.rds")
setwd("~/OneDrive_UofSC/Research/FG_revision/flexible_Gumbel_codes/figure3")
rm(list=ls())
source("../density.R")
library(HDInterval)
library(tidyverse)
library(kableExtra)
library(latex2exp)
ECM_est <- read_rds("../table2/ECM_est.rds")
ECM_SE <- read_rds("../table2/ECM_SE.rds")
Bayes_est <- read_rds("../table2/stanoutput.rds")
Bayes_summary <- summary(Bayes_est)
point.est.b <- Bayes_summary$summary[1:4,6] ##posterior median
df1 <- read.csv("../table2/data_toy.csv")
density_est <- density(df1$change,bw="SJ")
df_density <- data.frame(X = density_est$x,
value = density_est$y,
type = "KDE-SJ")
df_ECM <- data.frame(X = seq(min(df1$change),
max(df1$change),
length.out = 1000))
df_ECM$value <- mixture_gumbel_pdf(x=df_ECM$X,
w1=ECM_est$w1,
loc=ECM_est$loc,
scale1=ECM_est$scale1,
scale2=ECM_est$scale2)
df_ECM$type <- "ECM"
df_Bayes <- data.frame(X = seq(min(df1$change),
max(df1$change),
length.out = 1000))
df_Bayes$value <- mixture_gumbel_pdf(x=df_Bayes$X,
w1=point.est.b[1],
loc=point.est.b[2],
scale1=point.est.b[3],
scale2=point.est.b[4])
df_Bayes$type <- "Bayes"
## Normal mixture
est_normal <- readRDS("../table2/normal_mixture_EM.rds")
df_normal <- data.frame(X = seq(min(df1$change),
max(df1$change),
length.out = 1000))
df_normal$value <- est_normal$lambda[1]*
dnorm(df_normal$X,
mean=est_normal$mu[1],
sd=est_normal$sigma[1])+
est_normal$lambda[2]*
dnorm(df_normal$X,
mean=est_normal$mu[2],
sd=est_normal$sigma[2])
df_normal$type <- "normal mixture(EM)"
df_plot <- rbind(df_density,df_ECM,df_Bayes,df_normal)
df_plot$type <- factor(df_plot$type,levels = c("KDE-SJ","ECM","Bayes","normal mixture(EM)"))
p1 <- df_plot %>% ggplot(aes(x = X, y = value, linetype = type, color = type)) +
geom_line() +
theme_bw() +
theme(legend.position = "bottom") +
ylab("density") +
xlab("daily maximum water elevation change (in feet)") +
ggtitle("Three difference density estimation for same water elevation change data.")
p1
ggsave("densityplot-1.pdf",p1,height = 5,width = 7)
sum_fit$summary[,6][1:4]
sum_fit$summary[,6]
setwd("~/OneDrive_UofSC/Research/FG_revision/flexible_Gumbel_codes/table2")
fit <- readRDS("stanoutput.rds")
sum_fit <- summary(fit)
sum_fit$summary[,6][1:4]
sum_fit$summary[,6][1:4][1]
loglik_value <- sum(mixture_gumbel_pdf(x = df1$change,
w1 = sum_fit$summary[,6][1:4][1],
loc = sum_fit$summary[,6][1:4][2],
scale1 = sum_fit$summary[,6][1:4][3],
scale2 = sum_fit$summary[,6][1:4][4],
log=TRUE))
AIC_value <- -2*loglik_value + 2*(4) ##2506.028
BIC_value <- -2*loglik_value + 4*log(length(df1$change)) ##2521.638
AIC_value
BIC_value <- -2*loglik_value + 4*log(length(df1$change))
BIC_value
sum_fit$summary[,6][1:4]
sum_fit$summary[,6]
sum_fit
sum_fit$summary
setwd("~/OneDrive_UofSC/Research/FG_revision/flexible_Gumbel_codes/table2")
rm(list=ls())
library(HDInterval)
library(tidyverse)
library(kableExtra)
ECM_est <- read_rds("ECM_est.rds")
ECM_SE <- read_rds("ECM_SE.rds")
Bayes_est <- read_rds("stanoutput.rds")
Bayes_summary <- summary(Bayes_est)
point.est.b <- Bayes_summary$summary[1:4,6] ##posterior median
s.d.b <- Bayes_summary$summary[1:4,3] ##posterior standard deviation
Bayes_CI <- hdi(extract(Bayes_est))
Bayes_CI <- bind_rows(Bayes_CI)[1:4,]
Bayes_CI
sum_fit$summary
fit <- readRDS("stanoutput.rds")
sum_fit <- summary(fit)
sum_fit$summary[,6][1:4]
sum_fit$summary
round(sum_fit$summary,2)
setwd("~/OneDrive_UofSC/Research/FG_revision/flexible_Gumbel_codes/crime_data_table3_4_figure3")
rm(list = ls())
library(cmdstanr)
library(posterior)
library(bayesplot)
library(coda)
library(MASS)
library(tidyverse)
library(readxl)
library(gridExtra)
library(latex2exp)
library(parallel)
library(HDInterval)
color_scheme_set("brightblue")
## data import
df1 <- readxl::read_xlsx("./crime.xlsx")
y <- df1$`murder rate`
X <- as.matrix(df1[,c("college","poverty","metropolitan")])
N <- nrow(X)
P <- ncol(X)
## data list for stan
dat <- list(N = N,
P = P,
X = X,
y = y)
## FG reg
stan_file <- "../FG_MLR.stan"
stan_mod <- cmdstan_model(stan_file)
## MCMC
stan_fit <- stan_mod$sample(
data = dat,
seed = 100,
chains = 4,
parallel_chains = 4,
refresh = 0,
iter_warmup = 10000,
save_warmup = FALSE,
iter_sampling = 100000,
)
## parameter estimation
par.est <- stan_fit$summary(c("alpha",paste0("beta[",1:P,"]"),"scale1","scale2"))
print(par.est)
round(par.est,2)
round(par.est,2)
stan_fit$summary(c("alpha",paste0("beta[",1:P,"]"),"scale1","scale2"),~quantile(.x, probs = c(0.025, 0.25, 0.50, 0.75,0.975)))
print(par.est)
par.est
df1$`murder rate`
par.est$median
par.est
par.est <- stan_fit$summary(c("alpha",paste0("beta[",1:P,"]"),"scale1","scale2","w1"))
par.est
X
X %*% par.est$median[2,3,4] + par.est$median[1]
par.est$median[1]
X %*% par.est$median[2,3,4]
X
X %*% as.numeric(par.est$median[2,3,4]) + par.est$median[1]
X %*% as.numeric(par.est$median[2,3,4]) + as.numeric(par.est$median[1])
X %*% as.numeric(par.est$median[2,3,4]) + as.numeric(par.est$median[1])
X %*% as.numeric(par.est$median[2,3,4])
X
class(X)
as.matrix(X) %*% as.numeric(par.est$median[2,3,4]) + as.numeric(par.est$median[1])
as.matrix(X) %*% as.numeric(par.est$median[2,3,4])
as.matrix(X)
par.est$median[2,3,4]
as.matrix(X) %*% as.numeric(par.est$median[c(2,3,4)]) + as.numeric(par.est$median[1])
as.matrix(X) %*% par.est$median[c(2,3,4)] + par.est$median[1]
loc.est <- as.numeric(as.matrix(X) %*% par.est$median[c(2,3,4)] + par.est$median[1])
loglik_value <- sum(mixture_gumbel_pdf(x = df1$`murder rate`,
w1 = par.est$median[7],
loc = loc.est,
scale1 = par.est$median[5],
scale2 = par.est$median[6],
log=TRUE))
AIC_value <- -2*loglik_value + 2*(4) ##2506.028
AIC_value
par.est <- stan_fit$summary(c("alpha",paste0("beta[",1:P,"]"),"scale1","scale2","w1"))
loc.est <- as.numeric(as.matrix(X) %*% par.est$median[c(2,3,4)] + par.est$median[1])
loglik_value <- sum(mixture_gumbel_pdf(x = df1$`murder rate`,
w1 = par.est$median[7],
loc = loc.est,
scale1 = par.est$median[5],
scale2 = par.est$median[6],
log=TRUE))
AIC_value <- -2*loglik_value + 2*(4) ##2506.028
AIC_value
AIC_value
loglik_value
df1$`murder rate`
loc.est
par.est$median[5]
loglik_value <- sum(mixture_gumbel_pdf(x = as.numeric(df1$`murder rate`),
w1 = par.est$median[7],
loc = loc.est,
scale1 = par.est$median[5],
scale2 = par.est$median[6],
log=TRUE))
AIC_value <- -2*loglik_value + 2*(4) ##2506.028
AIC_value
sum(mixture_gumbel_pdf(x = as.numeric(df1$`murder rate`),
w1 = par.est$median[7],
loc = loc.est,
scale1 = par.est$median[5],
scale2 = par.est$median[6],
log=TRUE))
sum(mixture_gumbel_pdf(x = as.numeric(df1$`murder rate`),
w1 = par.est$median[7],
loc = loc.est,
scale1 = par.est$median[5],
scale2 = par.est$median[6],
log=TRUE))
as.numeric(df1$`murder rate`)
par.est$median[7]
loc.est
as.numeric(df1$`murder rate`)
length(as.numeric(df1$`murder rate`))
length(loc.est)
loglik_value <- sum(mixture_gumbel_pdf(x = as.numeric(df1$`murder rate`) - loc.est,
w1 = par.est$median[7],
loc = 0.0,
scale1 = par.est$median[5],
scale2 = par.est$median[6],
log=TRUE))
AIC_value <- -2*loglik_value + 2*(4) ##2506.028
AIC_value
print(loc.est)
print(AIC_value)
source("../density.R")
loglik_value <- sum(mixture_gumbel_pdf(x = as.numeric(df1$`murder rate`) - loc.est,
w1 = par.est$median[7],
loc = 0.0,
scale1 = par.est$median[5],
scale2 = par.est$median[6],
log=TRUE))
AIC_value <- -2*loglik_value + 2*(4) ##2506.028
AIC_value
AIC_value <- -2*loglik_value + 2*(7) ##233.3938
AIC_value
BIC_value <- -2*loglik_value + 7*log(length(df1$`murder rate`))
BIC_value
rm(list = ls())
library(cmdstanr)
library(posterior)
library(bayesplot)
library(coda)
library(MASS)
library(tidyverse)
library(readxl)
library(gridExtra)
library(latex2exp)
library(parallel)
library(HDInterval)
color_scheme_set("brightblue")
## data import
df1 <- readxl::read_xlsx("./crime.xlsx")
y <- df1$`murder rate`
X <- as.matrix(df1[,c("college","poverty","metropolitan")])
N <- nrow(X)
P <- ncol(X)
## data list for stan
dat <- list(N = N,
P = P,
X = X,
y = y)
## FG reg
stan_file <- "../FG_MLR.stan"
stan_mod <- cmdstan_model(stan_file)
## MCMC
stan_fit <- stan_mod$sample(
data = dat,
seed = 100,
chains = 4,
parallel_chains = 4,
refresh = 0,
iter_warmup = 10000,
save_warmup = FALSE,
iter_sampling = 100000,
)
## parameter estimation
par.est <- stan_fit$summary(c("alpha",paste0("beta[",1:P,"]"),"scale1","scale2"))
print(par.est)
stan_fit$summary(c("alpha",paste0("beta[",1:P,"]"),"scale1","scale2"),~quantile(.x, probs = c(0.025, 0.25, 0.50, 0.75,0.975)))
post_df <- stan_fit$draws(c("alpha",paste0("beta[",1:P,"]"),"scale1","scale2"), format = "df")
hdi(post_df$alpha)
hdi(post_df$`beta[1]`)
hdi(post_df$`beta[2]`)
hdi(post_df$`beta[3]`)
hdi(post_df$scale1)
hdi(post_df$scale2)
#mcmc_trace(post_df)
## normal reg
mean_reg <- lm(df1$`murder rate` ~ df1$college + df1$poverty + df1$metropolitan)
summary(mean_reg)
confint(mean_reg,level = 0.95)
## define negative loglikelihood of FG model
library(stats4)
source("../density.R")
designX <- cbind(1,X)
FG_neglog <- function(w1,sigma1,sigma2,alpha,beta1,beta2,beta3) {
beta <- as.vector(c(alpha,beta1,beta2,beta3))
eta <- as.numeric(designX %*% beta)
p1 <- w1*rgumbel_pdf(y,eta,sigma1,log=FALSE)
p2 <- (1-w1)*lgumbel_pdf(y,eta,sigma2,log=FALSE)
den_value <- p1 + p2
output <- -sum(log(den_value))
return(output)
}
FG_mle <- mle(FG_neglog,
start = list(w1 = 0.9,
sigma1 = 2,
sigma2 = 52,
alpha = -24,
beta1 = 0.5,
beta2 = 1,
beta3 = 0.05),
lower = c(1e-6,1e-6,1e-6,-Inf,-Inf,-Inf,-Inf),
upper = c(1,Inf,Inf,Inf,Inf,Inf,Inf))
summary_FG <- summary(FG_mle)
tab_FG <- data.frame(summary_FG@coef)
tab_FG$lower <- tab_FG$Estimate - qnorm(0.975)*tab_FG$Std..Error
tab_FG$upper <- tab_FG$Estimate + qnorm(0.975)*tab_FG$Std..Error
sapply(tab_FG, function(x){round(x,3)})
summary_FG
tab_FG
tab_FG[c(5,6,7),1]
# Frequentist model
loc.est <- as.numeric(as.matrix(X) %*% tab_FG[c(5,6,7),1] + tab_FG[4,1])
# Frequentist model
loc.est <- as.numeric(as.matrix(X) %*% tab_FG[c(5,6,7),1] + tab_FG[4,1])
loglik_value <- sum(mixture_gumbel_pdf(x = as.numeric(df1$`murder rate`) - loc.est,
w1 = tab_FG[1,1],
loc = 0.0,
scale1 = tab_FG[2,1],
scale2 = tab_FG[3,1],
log=TRUE))
AIC_value <- -2*loglik_value + 2*(7) ##233.3938
BIC_value <- -2*loglik_value + 7*log(length(df1$`murder rate`)) ##252.9166
AIC_value
BIC_value <- -2*loglik_value + 7*log(length(df1$`murder rate`)) ##252.9166
BIC_value
tab_FG
# Bayesian model
loglik_value <- sum(mixture_gumbel_pdf(x = as.numeric(df1$`murder rate`) - loc.est,
w1 = par.est$median[7],
loc = 0.0,
scale1 = par.est$median[5],
scale2 = par.est$median[6],
log=TRUE))
AIC_value <- -2*loglik_value + 2*(7) ##233.3938
AIC_value
# Bayesian model
loglik_value <- sum(mixture_gumbel_pdf(x = as.numeric(df1$`murder rate`) - loc.est,
w1 = par.est$median[7],
loc = 0.0,
scale1 = par.est$median[5],
scale2 = par.est$median[6],
log=TRUE))
AIC_value <- -2*loglik_value + 2*(7) ##233.3938
AIC_value
BIC_value <- -2*loglik_value + 7*log(length(df1$`murder rate`)) ##252.9166
BIC_value
par.est <- stan_fit$summary(c("alpha",paste0("beta[",1:P,"]"),"scale1","scale2","w1"))
loc.est <- as.numeric(as.matrix(X) %*% par.est$median[c(2,3,4)] + par.est$median[1])
source("../density.R")
# Bayesian model
loglik_value <- sum(mixture_gumbel_pdf(x = as.numeric(df1$`murder rate`) - loc.est,
w1 = par.est$median[7],
loc = 0.0,
scale1 = par.est$median[5],
scale2 = par.est$median[6],
log=TRUE))
AIC_value <- -2*loglik_value + 2*(7) ##238.7104
BIC_value <- -2*loglik_value + 7*log(length(df1$`murder rate`)) ##252.9166
AIC_value
BIC_value <- -2*loglik_value + 7*log(length(df1$`murder rate`)) ##252.9166
BIC_value
par.est <- stan_fit$summary(c("alpha",paste0("beta[",1:P,"]"),"scale1","scale2","w1"))
loc.est <- as.numeric(as.matrix(X) %*% par.est$median[c(2,3,4)] + par.est$median[1])
source("../density.R")
# Bayesian model
loglik_value <- sum(mixture_gumbel_pdf(x = as.numeric(df1$`murder rate`) - loc.est,
w1 = par.est$median[7],
loc = 0.0,
scale1 = par.est$median[5],
scale2 = par.est$median[6],
log=TRUE))
AIC_value <- -2*loglik_value + 2*(7) ##239.3938
BIC_value <- -2*loglik_value + 7*log(length(df1$`murder rate`)) ##252.9166
AIC_value
BIC_value
loc.est <- as.numeric(as.matrix(X) %*% tab_FG[c(5,6,7),1] + tab_FG[4,1])
loglik_value <- sum(mixture_gumbel_pdf(x = as.numeric(df1$`murder rate`) - loc.est,
w1 = tab_FG[1,1],
loc = 0.0,
scale1 = tab_FG[2,1],
scale2 = tab_FG[3,1],
log=TRUE))
AIC_value <- -2*loglik_value + 2*(7) ##238.7104
BIC_value <- -2*loglik_value + 7*log(length(df1$`murder rate`)) ##252.2332
AIC_value
BIC_value
